{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import necessary libraries such as numpy, tensorflow, keras, matplotlib, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "from tensorflow.keras import backend as K\n",
    "import segmentation_models_3D as sm\n",
    "from keras.metrics import MeanIoU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "import splitfolders\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tifffile import imsave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constants and Paths\n",
    "Define constants and paths for training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Constants and Paths\n",
    "\n",
    "# Define paths for training and validation datasets\n",
    "TRAIN_DATASET_PATH = r'C:\\Users\\yagiz\\OneDrive\\Masa端st端\\kodlar\\UnetsegmentationDeneme\\unet-segmentation-project\\data\\train\\BraTS2020_TrainingData\\MICCAI_BraTS2020_TrainingData'\n",
    "VALIDATION_DATASET_PATH = r'C:\\Users\\yagiz\\OneDrive\\Masa端st端\\kodlar\\UnetsegmentationDeneme\\unet-segmentation-project\\data\\validation\\BraTS2020_ValidationData\\MICCAI_BraTS2020_ValidationData'\n",
    "\n",
    "# Define paths for input images and masks for training\n",
    "TRAIN_IMG_DIR = os.path.join(TRAIN_DATASET_PATH, \"input_data_3channels/images/\")\n",
    "TRAIN_MASK_DIR = os.path.join(TRAIN_DATASET_PATH, \"input_data_3channels/masks/\")\n",
    "\n",
    "# Define paths for input images for validation\n",
    "VAL_IMG_DIR = os.path.join(VALIDATION_DATASET_PATH, \"input_data_3channels/images/\")\n",
    "\n",
    "# Define batch size for training and validation\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Data\n",
    "Load and preprocess the data, including scaling and reshaping images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess Data\n",
    "\n",
    "# Function to load .npy files from a given directory\n",
    "def load_img(img_dir, img_list):\n",
    "    images = []\n",
    "    for image_name in img_list:\n",
    "        if image_name.endswith('.npy'):  # Only load .npy files\n",
    "            image = np.load(os.path.join(img_dir, image_name)).astype(np.float32)\n",
    "            images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "# Function to load and preprocess images and masks in batches\n",
    "def imageLoader(img_dir, img_list, mask_dir, mask_list, batch_size, dtype=np.float32):\n",
    "    L = len(img_list)\n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < L:\n",
    "            limit = min(batch_end, L)\n",
    "            X = load_img(img_dir, img_list[batch_start:limit])  # Load images\n",
    "            Y = load_img(mask_dir, mask_list[batch_start:limit])  # Load masks\n",
    "            yield (X, Y)  # Return a tuple containing two numpy arrays\n",
    "            batch_start += batch_size\n",
    "            batch_end += batch_size\n",
    "\n",
    "# Function to load and preprocess validation images in batches\n",
    "def val_imageLoader(img_dir, img_list, batch_size, dtype=np.float32):\n",
    "    L = len(img_list)\n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < L:\n",
    "            limit = min(batch_end, L)\n",
    "            X = load_img(img_dir, img_list[batch_start:limit])  # Load images\n",
    "            yield X  # Return only images\n",
    "            batch_start += batch_size\n",
    "            batch_end += batch_size\n",
    "\n",
    "# Initialize data generators for training and validation\n",
    "train_img_list = os.listdir(TRAIN_IMG_DIR)\n",
    "train_mask_list = os.listdir(TRAIN_MASK_DIR)\n",
    "val_img_list = os.listdir(VAL_IMG_DIR)\n",
    "\n",
    "train_img_datagen = imageLoader(TRAIN_IMG_DIR, train_img_list, TRAIN_MASK_DIR, train_mask_list, BATCH_SIZE)\n",
    "val_img_datagen = val_imageLoader(VAL_IMG_DIR, val_img_list, BATCH_SIZE)\n",
    "\n",
    "# Test the data generator\n",
    "img, msk = next(train_img_datagen)\n",
    "\n",
    "# Visualize a random image and its corresponding mask from the batch\n",
    "img_num = random.randint(0, img.shape[0] - 1)\n",
    "test_img = img[img_num]\n",
    "test_mask = msk[img_num]\n",
    "test_mask = np.argmax(test_mask, axis=3)\n",
    "\n",
    "n_slice = random.randint(0, test_mask.shape[2])\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(231)\n",
    "plt.imshow(test_img[:, :, n_slice, 0], cmap='gray')\n",
    "plt.title('Image flair')\n",
    "plt.subplot(232)\n",
    "plt.imshow(test_img[:, :, n_slice, 1], cmap='gray')\n",
    "plt.title('Image t1')\n",
    "plt.subplot(233)\n",
    "plt.imshow(test_img[:, :, n_slice, 2], cmap='gray')\n",
    "plt.title('Image t1ce')\n",
    "plt.subplot(234)\n",
    "plt.imshow(test_img[:, :, n_slice, 3], cmap='gray')\n",
    "plt.title('Image t2')\n",
    "plt.subplot(235)\n",
    "plt.imshow(test_mask[:, :, n_slice])\n",
    "plt.title('Mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator Functions\n",
    "Define functions to load images and masks in batches for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load .npy files from a given directory\n",
    "def load_img(img_dir, img_list):\n",
    "    images = []\n",
    "    for image_name in img_list:\n",
    "        if image_name.endswith('.npy'):  # Only load .npy files\n",
    "            image = np.load(os.path.join(img_dir, image_name)).astype(np.float32)\n",
    "            images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "# Function to load and preprocess images and masks in batches\n",
    "def imageLoader(img_dir, img_list, mask_dir, mask_list, batch_size, dtype=np.float32):\n",
    "    L = len(img_list)\n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < L:\n",
    "            limit = min(batch_end, L)\n",
    "            X = load_img(img_dir, img_list[batch_start:limit])  # Load images\n",
    "            Y = load_img(mask_dir, mask_list[batch_start:limit])  # Load masks\n",
    "            yield (X, Y)  # Return a tuple containing two numpy arrays\n",
    "            batch_start += batch_size\n",
    "            batch_end += batch_size\n",
    "\n",
    "# Function to load and preprocess validation images in batches\n",
    "def val_imageLoader(img_dir, img_list, batch_size, dtype=np.float32):\n",
    "    L = len(img_list)\n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < L:\n",
    "            limit = min(batch_end, L)\n",
    "            X = load_img(img_dir, img_list[batch_start:limit])  # Load images\n",
    "            yield X  # Return only images\n",
    "            batch_start += batch_size\n",
    "            batch_end += batch_size\n",
    "\n",
    "# Initialize data generators for training and validation\n",
    "train_img_list = os.listdir(TRAIN_IMG_DIR)\n",
    "train_mask_list = os.listdir(TRAIN_MASK_DIR)\n",
    "val_img_list = os.listdir(VAL_IMG_DIR)\n",
    "\n",
    "train_img_datagen = imageLoader(TRAIN_IMG_DIR, train_img_list, TRAIN_MASK_DIR, train_mask_list, BATCH_SIZE)\n",
    "val_img_datagen = val_imageLoader(VAL_IMG_DIR, val_img_list, BATCH_SIZE)\n",
    "\n",
    "# Test the data generator\n",
    "img, msk = next(train_img_datagen)\n",
    "\n",
    "# Visualize a random image and its corresponding mask from the batch\n",
    "img_num = random.randint(0, img.shape[0] - 1)\n",
    "test_img = img[img_num]\n",
    "test_mask = msk[img_num]\n",
    "test_mask = np.argmax(test_mask, axis=3)\n",
    "\n",
    "n_slice = random.randint(0, test_mask.shape[2])\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(231)\n",
    "plt.imshow(test_img[:, :, n_slice, 0], cmap='gray')\n",
    "plt.title('Image flair')\n",
    "plt.subplot(232)\n",
    "plt.imshow(test_img[:, :, n_slice, 1], cmap='gray')\n",
    "plt.title('Image t1')\n",
    "plt.subplot(233)\n",
    "plt.imshow(test_img[:, :, n_slice, 2], cmap='gray')\n",
    "plt.title('Image t1ce')\n",
    "plt.subplot(234)\n",
    "plt.imshow(test_img[:, :, n_slice, 3], cmap='gray')\n",
    "plt.title('Image t2')\n",
    "plt.subplot(235)\n",
    "plt.imshow(test_mask[:, :, n_slice])\n",
    "plt.title('Mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data\n",
    "Visualize the images and masks to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Data\n",
    "\n",
    "# Visualize a random image and its corresponding mask from the batch\n",
    "img_num = random.randint(0, img.shape[0] - 1)\n",
    "test_img = img[img_num]\n",
    "test_mask = msk[img_num]\n",
    "test_mask = np.argmax(test_mask, axis=3)\n",
    "\n",
    "n_slice = random.randint(0, test_mask.shape[2])\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(231)\n",
    "plt.imshow(test_img[:, :, n_slice, 0], cmap='gray')\n",
    "plt.title('Image flair')\n",
    "plt.subplot(232)\n",
    "plt.imshow(test_img[:, :, n_slice, 1], cmap='gray')\n",
    "plt.title('Image t1')\n",
    "plt.subplot(233)\n",
    "plt.imshow(test_img[:, :, n_slice, 2], cmap='gray')\n",
    "plt.title('Image t1ce')\n",
    "plt.subplot(234)\n",
    "plt.imshow(test_img[:, :, n_slice, 3], cmap='gray')\n",
    "plt.title('Image t2')\n",
    "plt.subplot(235)\n",
    "plt.imshow(test_mask[:, :, n_slice])\n",
    "plt.title('Mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model and Training Functions\n",
    "Define the U-Net model, loss functions, metrics, and other training-related functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model and Training Functions\n",
    "\n",
    "# Define the U-Net model\n",
    "from Unet_model import simple_unet_model\n",
    "\n",
    "# Define loss functions\n",
    "wt0, wt1, wt2, wt3 = np.float32(0.25), np.float32(0.25), np.float32(0.25), np.float32(0.25)\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=np.array([wt0, wt1, wt2, wt3], dtype=np.float32))\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + (1 * focal_loss)\n",
    "\n",
    "# Define custom metrics\n",
    "def dice_metric(y_true, y_pred):\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    return (2. * intersection) / (K.sum(y_true) + K.sum(y_pred) + K.epsilon())\n",
    "\n",
    "def iou_metric(y_true, y_pred):\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    union = K.sum(y_true) + K.sum(y_pred) - intersection\n",
    "    return intersection / (union + K.epsilon())\n",
    "\n",
    "def f1_score_metric(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(K.round(y_pred), 'float32')\n",
    "    tp = K.sum(y_true * y_pred)\n",
    "    fp = K.sum(y_pred) - tp\n",
    "    fn = K.sum(y_true) - tp\n",
    "    return 2 * tp / (2 * tp + fp + fn + K.epsilon())\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(K.round(y_pred), 'float32')\n",
    "    tp = K.sum(y_true * y_pred)\n",
    "    fp = K.sum(y_pred) - tp\n",
    "    return tp / (tp + fp + K.epsilon())\n",
    "\n",
    "def recall_metric(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(K.round(y_pred), 'float32')\n",
    "    tp = K.sum(y_true * y_pred)\n",
    "    fn = K.sum(y_true) - tp\n",
    "    return tp / (tp + fn + K.epsilon())\n",
    "\n",
    "# Define function to calculate all metrics\n",
    "def calculate_all_metrics(y_true, y_pred):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    \n",
    "    f1 = f1_score(y_true_flat, y_pred_flat, average='weighted')\n",
    "    precision = precision_score(y_true_flat, y_pred_flat, average='weighted', zero_division=0)\n",
    "    sensitivity = recall_score(y_true_flat, y_pred_flat, average='weighted', zero_division=0)\n",
    "    \n",
    "    tn = np.sum((y_true_flat == 0) & (y_pred_flat == 0))\n",
    "    fp = np.sum((y_true_flat == 0) & (y_pred_flat == 1))\n",
    "    specificity = tn / (tn + fp + 1e-7)\n",
    "    \n",
    "    intersection = np.sum((y_true_flat == 1) & (y_pred_flat == 1))\n",
    "    union = np.sum((y_true_flat == 1) | (y_pred_flat == 1))\n",
    "    iou = intersection / (union + 1e-7)\n",
    "    \n",
    "    dice = 2 * intersection / (np.sum(y_true_flat) + np.sum(y_pred_flat) + 1e-7)\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'iou': iou,\n",
    "        'dice': dice\n",
    "    }\n",
    "\n",
    "# Define function to plot metrics\n",
    "def plot_metrics(history):\n",
    "    for metric_name in history.keys():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history[metric_name], 'y', label=f'Train {metric_name}')\n",
    "        if f'val_{metric_name}' in history:\n",
    "            plt.plot(history[f'val_{metric_name}'], 'r', label=f'Validation {metric_name}')\n",
    "        plt.title(f'Training and Validation {metric_name}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def plot_precision_recall_f1(history):\n",
    "    if 'precision_metric' in history and 'recall_metric' in history and 'f1_score_metric' in history:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history['recall_metric'], history['precision_metric'], 'b', label='Precision vs Recall')\n",
    "        plt.plot(history['recall_metric'], history['f1_score_metric'], 'g', label='F1 Score vs Recall')\n",
    "        plt.title('Precision and F1 Score vs Recall')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Define learning rate and optimizer\n",
    "LR = 0.0001\n",
    "optim = keras.optimizers.Adam(LR)\n",
    "\n",
    "# Define metrics\n",
    "metrics = [\n",
    "    'accuracy',\n",
    "    sm.metrics.IOUScore(threshold=0.5),\n",
    "    sm.metrics.FScore(threshold=0.5),\n",
    "    precision_metric,\n",
    "    recall_metric,\n",
    "    f1_score_metric,\n",
    "    iou_metric,\n",
    "    dice_metric\n",
    "]\n",
    "\n",
    "# Initialize U-Net model\n",
    "model = simple_unet_model(IMG_HEIGHT=128, IMG_WIDTH=128, IMG_DEPTH=128, IMG_CHANNELS=3, num_classes=4)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optim, loss=total_loss, metrics=metrics)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Train the U-Net model using the defined data generators and visualize the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "# Calculate steps per epoch for training and validation\n",
    "steps_per_epoch = len(train_img_list) // BATCH_SIZE\n",
    "val_steps_per_epoch = len(val_img_list) // BATCH_SIZE\n",
    "\n",
    "# Train the model\n",
    "epochs = 1\n",
    "history = model.fit(train_img_datagen,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=val_img_datagen,\n",
    "                    validation_steps=val_steps_per_epoch)\n",
    "\n",
    "# Save the trained model\n",
    "model_filename = f'saved_models/brats_3d_{epochs}epochs_simple_unet_weighted_dice.hdf5'\n",
    "model.save(model_filename)\n",
    "\n",
    "# Plot training and validation metrics\n",
    "plot_metrics(history.history)\n",
    "\n",
    "# Plot precision, recall, and F1 score\n",
    "plot_precision_recall_f1(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the trained model on validation data and calculate metrics such as IoU, Dice coefficient, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "\n",
    "# Load the trained model\n",
    "model_filename = f'saved_models/brats_3d_{epochs}epochs_simple_unet_weighted_dice.hdf5'\n",
    "my_model = load_model(model_filename, \n",
    "                      custom_objects={'dice_loss_plus_1focal_loss': total_loss,\n",
    "                                      'iou_score': sm.metrics.IOUScore(threshold=0.5), \n",
    "                                      'f_score': sm.metrics.FScore(threshold=0.5),\n",
    "                                      'iou_metric': iou_metric,\n",
    "                                      'dice_metric': dice_metric,\n",
    "                                      'f1_score_metric': f1_score_metric,\n",
    "                                      'precision_metric': precision_metric,\n",
    "                                      'recall_metric': recall_metric})\n",
    "my_model.compile(optimizer=keras.optimizers.Adam(LR), loss=total_loss, metrics=metrics)\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "test_img_datagen = imageLoader(VAL_IMG_DIR, val_img_list, \n",
    "                               TRAIN_MASK_DIR, train_mask_list, BATCH_SIZE)\n",
    "\n",
    "# Get a batch of validation images and masks\n",
    "test_image_batch, test_mask_batch = next(test_img_datagen)\n",
    "\n",
    "# Predict on the validation batch\n",
    "test_mask_batch_argmax = np.argmax(test_mask_batch, axis=4)\n",
    "test_pred_batch = my_model.predict(test_image_batch)\n",
    "test_pred_batch_argmax = np.argmax(test_pred_batch, axis=4)\n",
    "\n",
    "# Calculate Mean IoU\n",
    "n_classes = 4\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)\n",
    "IOU_keras.update_state(test_pred_batch_argmax, test_mask_batch_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "# Visualize predictions on a random validation image\n",
    "img_num = random.randint(0, test_image_batch.shape[0] - 1)\n",
    "test_img = test_image_batch[img_num]\n",
    "test_mask = test_mask_batch_argmax[img_num]\n",
    "test_pred = test_pred_batch_argmax[img_num]\n",
    "\n",
    "n_slice = random.randint(0, test_mask.shape[2])\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Validation Image')\n",
    "plt.imshow(test_img[:, :, n_slice, 0], cmap='gray')\n",
    "plt.subplot(232)\n",
    "plt.title('Validation Mask')\n",
    "plt.imshow(test_mask[:, :, n_slice])\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on Validation Image')\n",
    "plt.imshow(test_pred[:, :, n_slice])\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print all metrics\n",
    "metrics = calculate_all_metrics(test_mask, test_pred)\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f'{metric_name}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Predictions\n",
    "Visualize the model's predictions on test images and compare them with the ground truth masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Predictions\n",
    "\n",
    "# Function to preprocess image\n",
    "def preprocess_image(image_path, target_size):\n",
    "    if image_path.endswith('.npy'):\n",
    "        img = np.load(image_path)\n",
    "    else:\n",
    "        img = Image.open(image_path)\n",
    "        img = img.resize(target_size)\n",
    "        img = np.array(img)\n",
    "    \n",
    "    img = img.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "# Function to predict and visualize\n",
    "def predict_and_visualize(model, image_path, mask_path=None, target_size=(128, 128, 128)):\n",
    "    # Preprocess the image\n",
    "    img = preprocess_image(image_path, target_size)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(img)\n",
    "    prediction_argmax = np.argmax(prediction, axis=4)[0, :, :, :]\n",
    "    \n",
    "    # Visualize the results\n",
    "    n_slice = prediction_argmax.shape[2] // 2  # Select middle slice for visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.title('Original Image')\n",
    "    plt.imshow(img[0, :, :, n_slice, 0], cmap='gray')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.title('Predicted Mask')\n",
    "    plt.imshow(prediction_argmax[:, :, n_slice])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # If mask is provided, calculate and print metrics\n",
    "    if mask_path:\n",
    "        mask = preprocess_image(mask_path, target_size)\n",
    "        mask_argmax = np.argmax(mask, axis=4)[0, :, :, :]\n",
    "        metrics = calculate_all_metrics(mask_argmax, prediction_argmax)\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f'{metric_name}: {value:.4f}')\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Example usage\n",
    "image_path = os.path.join(VAL_IMG_DIR, 'image_0.npy')  # Update with your image path\n",
    "mask_path = os.path.join(TRAIN_MASK_DIR, 'mask_0.npy')  # Update with your mask path if available\n",
    "metrics = predict_and_visualize(my_model, image_path, mask_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
